{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51d34e48",
   "metadata": {},
   "source": [
    "# Лекция №5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1721ae3f-27c1-4d01-90c3-a4546e5e0d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d11a57",
   "metadata": {},
   "source": [
    "## Задачи DL кроме классификации(модификации)\n",
    "\n",
    "- классификация+локализация(определить тип и показать расположение)\n",
    "- семантическая сегментация(определение принадлежности пикселя тому или иному объекту)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503b2652",
   "metadata": {},
   "source": [
    "### Локализация + классификация\n",
    "\n",
    "- кроме возвращаемой метки класса слои выделяют расположение объекта\n",
    "- детектирующие слои работают с выходами сверток до этапа классификации"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0ba383",
   "metadata": {},
   "source": [
    "### Метрика качества сегментации(детекторов)\n",
    "\n",
    "- __IoU__: $IoU = \\frac{|A\\cap B|}{|A\\cup B|} \\ge \\alpha$\n",
    "- __MeanAvragePrecision__:\n",
    "  - берем один класс;\n",
    "  - смотрим, как много объектов этого класса выбраны алгоритмом правильно;\n",
    "  - считаем prec, recall для каждой строки(объекта данных);\n",
    "  - по результатам строим кривую, затем считаем площадь под графиком;\n",
    "  - получаем качества для всех классов из датасета, усредняем и получаем MAP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298a33ea",
   "metadata": {},
   "source": [
    "### Детектирование объектов\n",
    "\n",
    "- __Selective search__:\n",
    "  1. разбиваем изображение на сегменты(например, кластеризируем);\n",
    "  2. каждый сегмент выделяем в рамку(получаем много рамок);\n",
    "  3. сегменты объединяем в большие сегменты;\n",
    "  4. повторяем п.2-3;\n",
    "  5. получаем разбиение.\n",
    "\n",
    "#### R-CNN\n",
    "\n",
    "__Изначальная вариация__:\n",
    "- Исходная картинка пропускается через SelectiveSearch;\n",
    "- Убрали рамку, растянули изображение до нужных размеров($224\\times224$);\n",
    "- Классифицировали объект+SVM(+regression для коррекции SelectiveSearch);\n",
    "\n",
    "__Дообучение на конкретном датасете__:\n",
    "- Рассматривается объект на изображении;\n",
    "- Если IoU>30%, то объект - машина(например), иначе - фон;\n",
    "\n",
    "__BBox regression__:\n",
    "- Известны параметры рамки(левый верхний угол и высота+ширина);\n",
    "- Ошибка - как сильно ошиблись в выборе середины рамки, ширины и высоты;\n",
    "\n",
    "__Недостатки__:\n",
    "- Долгое обучение;\n",
    "- CNN не для всего изображения, а для сегментов(много запусков).\n",
    "\n",
    "__NMS__:\n",
    "- Выбирается класс, берутся степени уверенности bbox'ов;\n",
    "- Из них выбираются наибольшие уверенности:\n",
    "  - Если пересечение большое, то новую рамку(меньше уверенность) зануляем(игнорируем);\n",
    "  - Если пересечение незначительное, то оставляем рамку."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e57ff52",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76b4275",
   "metadata": {},
   "source": [
    "#### SPP-net: Spatial Pyramid Pooling\n",
    "\n",
    "- Проблема классического детектирования - выбираем фиксированные размеры рамки, переводим в нужное разрешение, передаем в слои;\n",
    "- __Можно__ поменять последовательность: передать изображение в слои и затем применить __pyramid pooling__.\n",
    "\n",
    "__SPP__:\n",
    "- Применим разное количество делений изображения(исходное, 4 части, 16 частей, например), затем собрать полученные данные и передать в сеть. В результате __работает быстрее__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "599a0c9f-b131-4ebb-a673-b36f745094b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Теперь есть adaptive pooling, \n",
    "# показывая желаемую размерность на выходе.\n",
    "def spatial_pyramid_pool(self,previous_conv, num_sample, previous_conv_size, out_pool_size):\n",
    "    '''\n",
    "    previous_conv: a tensor vector of previous convolution layer\n",
    "    num_sample: an int number of image in the batch\n",
    "    previous_conv_size: an int vector [height, width] of the matrix features size of previous convolution layer\n",
    "    out_pool_size: a int vector of expected output size of max pooling layer\n",
    "    returns: a tensor vector with shape [1 x n] is the concentration of multi-level pooling\n",
    "    '''\n",
    "    for i in range(len(out_pool_size)):\n",
    "        h_wid = int(math.ceil(previous_conv_size[0] / out_pool_size[i]))\n",
    "        w_wid = int(math.ceil(previous_conv_size[1] / out_pool_size[i]))\n",
    "        h_pad = (h_wid*out_pool_size[i] - previous_conv_size[0] + 1)/2\n",
    "        w_pad = (w_wid*out_pool_size[i] - previous_conv_size[1] + 1)/2\n",
    "        maxpool = nn.MaxPool2d((h_wid, w_wid), stride=(h_wid, w_wid), padding=(h_pad, w_pad))\n",
    "        x = maxpool(previous_conv)\n",
    "        if(i == 0):\n",
    "            spp = x.view(num_sample,-1)\n",
    "        else:\n",
    "            spp = torch.cat((spp, x.view(num_sample,-1)), 1)\n",
    "    return spp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2c653d",
   "metadata": {},
   "source": [
    "#### Fast R-CNN\n",
    "\n",
    "- R-CNN + SPP;\n",
    "- Ошибка - сумма ошибок классификации и детекции;\n",
    "- Пропукалось изображение через CNN, выделялась рамка, затем к ней применялись RoIPooling, передаем в FC.\n",
    "\n",
    "#### Faster R-CNN\n",
    "\n",
    "- Убрали SelectiveSearch => RPN;\n",
    "\n",
    "__RPN__:\n",
    "- Выберем anchors для детекции, полагая, что именно они и только они могут быть детектированы;\n",
    "- После CNN:\n",
    "  - Для каждой ячейки в карте отклика нарисуем k anchors:\n",
    "    - Квадрат, прямоугольник, повернутый прямоугольник, и т.д.;\n",
    "    - Для каждой точки строим это множество возможных bbox'ов;\n",
    "  - Для каждого региона сеть определяет: есть объект или нет + какая рамка у объекта.\n",
    "- __Пример__: определить кота и дерево на снимке:\n",
    "  1.  Разбиваем RoIPooling'ом изображение на 2 на 3 таблицу;\n",
    "  2.  Каждой ячейке приписываем весь набор anchor'ов;\n",
    "  3.  Нашли bbox, которые сильно пересекаются с изначальной разметкой => RPN говорит, что объект здесь + bbox в ходе регрессии может расширяться\n",
    "\n",
    "- __RPN__ не заостряет внимание на объекте и старается просто искать рамку, а __после классификации рамка уже уточняется специально для каждого класса__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05d5f34e-b9b8-4e29-81e9-f4fa22ed618b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RPN(nn.Module):\n",
    "    def __init__(self, in_channels=512, mid_channels=512, n_anchor=9):\n",
    "        super(RPN, self).__init__()\n",
    "        self.mid_channels = mid_channels\n",
    "        self.in_channels = in_channels # depends on the output feature map. in vgg 16 it is equal to 512\n",
    "        self.n_anchor = n_anchor # Number of anchors at each location\n",
    "        self.conv1 = nn.Conv2d(self.in_channels, self.mid_channels, 3, 1, 1)\n",
    "        self.reg_layer = nn.Conv2d(mid_channels, n_anchor *4, 1, 1, 0) \n",
    "        self.cls_layer = nn.Conv2d(mid_channels, n_anchor *2, 1, 1, 0)\n",
    "        \n",
    "        self.conv1.weight.data.normal_(0, 0.01) # conv sliding layer\n",
    "        self.conv1.bias.data.zero_()\n",
    "        self.reg_layer.weight.data.normal_(0, 0.01) # Regression layer\n",
    "        self.reg_layer.bias.data.zero_()\n",
    "        self.cls_layer.weight.data.normal_(0, 0.01) # classification layer\n",
    "        self.cls_layer.bias.data.zero_()\n",
    "    \n",
    "    def forward(self, k):\n",
    "        bat_num = k.shape[0]\n",
    "        x = self.conv1(k)\n",
    "        pred_anchor_locs = self.reg_layer(x)\n",
    "        pred_cls_scores = self.cls_layer(x)\n",
    "        \n",
    "        pred_anchor_locs = pred_anchor_locs.permute(0, 2, 3, 1).contiguous().view(bat_num, -1, 4)\n",
    "        pred_cls_scores = pred_cls_scores.permute(0, 2, 3, 1).contiguous()\n",
    "        objectness_score = pred_cls_scores.view(bat_num, 50, 50, 9, 2)[:, :, :, :, 1].contiguous().view(bat_num, -1)\n",
    "        pred_cls_scores  = pred_cls_scores.view(bat_num, -1, 2)\n",
    "        \n",
    "        return pred_anchor_locs, pred_cls_scores, objectness_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8502ef36",
   "metadata": {},
   "source": [
    "#### YouOnlyLookOnce\n",
    "\n",
    "- Одна CNN;\n",
    "- За один прямой проход находит объекты и их классы;\n",
    "\n",
    "1. Берем изображение, делим на одинаковые объекты($7\\times7$);\n",
    "2. Предсказываем регионы, размеры bbox'а и класс;\n",
    "3. В результате работы получаем тензор $7\\times7\\times30$:\n",
    "   - первые 5 значений для 1-го класса;\n",
    "   - вторые 5 для второго;\n",
    "   - далее вероятность классов в данной ячейке с меньшими уверенностями.\n",
    "4. Получаем принадлежность рамки тому или иному объекту(классу);\n",
    "5. Детектируем те ячейки, центры которых попали в данную рамку;\n",
    "\n",
    "__Недостатки__:\n",
    "- В каждой ячейке центры максимум 2-х классов.\n",
    "\n",
    "__+__:\n",
    "- Быстрее работает.\n",
    "\n",
    "#### YOLO v2\n",
    "\n",
    "- SkipConnections;\n",
    "- Убрали FC слои;\n",
    "- Anchor'ы определяли с помощью k-means.\n",
    "\n",
    "#### YOLO v3\n",
    "\n",
    "- Детекция на разных масштаба(не только $7\\times7$);\n",
    "- Каскадная схема(чем глубже слой, тем сильнее дробление изображения);\n",
    "\n",
    "#### SSD\n",
    "\n",
    "> С чего YOLO v3 взяла идею\n",
    "\n",
    "- Детекция сеток разного масштаба;\n",
    "- Рассматривались по-прежнему фиксированные наборы anchor-box'ов;\n",
    "- Находим, кто изображен и как меняется bbox;\n",
    "- Для каждой ячейки(масштаба карты) определен набор bbox'ов;\n",
    "- Помогает детектировать объекты разных размеров;\n",
    "- В итоговом тензоре больше размерность(последняя), т.к. показывает уверенность bbox'ов разных размеров;\n",
    "\n",
    "#### FPN\n",
    "\n",
    "- Иерархический подход к детекции(разные объекты разных размеров);\n",
    "- __Вариации__:\n",
    "  - Каждый слой(изменить изображение, передать сети) дает свое предсказание;\n",
    "  - Предсказание берется только с последнего слоя(причем через сеть проходит только исходное изображение);\n",
    "  - SSD-вариант(иерархия с исходным изображением(т.е. не меняются размеры изображения), причем предсказание с каждого слоя);\n",
    "  - SSD-вариант, но с обратным уточнением предсказаний, причем с каждого слоя берется предсказание;\n",
    "  - Как и в предыдущем варианте, только предсказание с последнего слоя(наибольший размер карты) - U-Net."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14bf5ae",
   "metadata": {},
   "source": [
    "### Новые подходы\n",
    "\n",
    "#### Anchor-Free[без якорей]\n",
    "\n",
    "- Пусть на изображении несколько объектов;\n",
    "- Рамку определяют два угла(левый верхний и нижний правый);\n",
    "- __Пусть__ каждый пиксель определяет, является ли он углом - тогда не нужны якоря(CornerNet; _CenterNet_ - центры рамки);\n",
    "    - Изменился Pooling: \n",
    "      - Не все объекты в левом верхнем углу содержат точки объекта, поэтому заполняем сокращенную карту признаков максимумами в строке и в столбце;\n",
    "    - __CornerNet__:\n",
    "      - Предсказываются углы рамки и вероятность быть углом;\n",
    "      - Embedding - кодируем(и характерные им углы) объекты для разделимости;\n",
    "      - Offsets - смещение рамки для большей точности;\n",
    "      - Больше одной точки считаются углом(окрестность).\n",
    "\n",
    "#### FCOS\n",
    "\n",
    "- Качество лучше большинства алгоритмов детекции.\n",
    "\n",
    "- Пусть не углы определяются, а принадлежность пикселя объекту:\n",
    "  - __Недостаток__: если несколько объектов, то одна точка может быть в двух объектах:\n",
    "    - Если такая ситуация случилась, предпочтение минимальному региону.\n",
    "- В каждом пикселе предсказываем расстояние до границ объекта и метка класса и вероятность, что объект в центре.\n",
    "\n",
    "__Архитектура__:\n",
    "- Как в _двунаправленной пирамиде_ + детекция(отдельно в head) - классификация, центральность, bbox."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b54d8a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.14 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
