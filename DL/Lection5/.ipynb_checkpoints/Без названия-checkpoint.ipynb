{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1721ae3f-27c1-4d01-90c3-a4546e5e0d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe58c31-8215-4c49-9ba7-9d7ab786e2fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed2024d-56c1-497a-b6e4-ffea05035d8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331248fd-f187-4c31-8225-d000bcda3dca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "599a0c9f-b131-4ebb-a673-b36f745094b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def spatial_pyramid_pool(self,previous_conv, num_sample, previous_conv_size, out_pool_size):\n",
    "    '''\n",
    "    previous_conv: a tensor vector of previous convolution layer\n",
    "    num_sample: an int number of image in the batch\n",
    "    previous_conv_size: an int vector [height, width] of the matrix features size of previous convolution layer\n",
    "    out_pool_size: a int vector of expected output size of max pooling layer\n",
    "    returns: a tensor vector with shape [1 x n] is the concentration of multi-level pooling\n",
    "    '''\n",
    "    for i in range(len(out_pool_size)):\n",
    "        h_wid = int(math.ceil(previous_conv_size[0] / out_pool_size[i]))\n",
    "        w_wid = int(math.ceil(previous_conv_size[1] / out_pool_size[i]))\n",
    "        h_pad = (h_wid*out_pool_size[i] - previous_conv_size[0] + 1)/2\n",
    "        w_pad = (w_wid*out_pool_size[i] - previous_conv_size[1] + 1)/2\n",
    "        maxpool = nn.MaxPool2d((h_wid, w_wid), stride=(h_wid, w_wid), padding=(h_pad, w_pad))\n",
    "        x = maxpool(previous_conv)\n",
    "        if(i == 0):\n",
    "            spp = x.view(num_sample,-1)\n",
    "        else:\n",
    "            spp = torch.cat((spp, x.view(num_sample,-1)), 1)\n",
    "    return spp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b176a9-2d77-479c-98a9-dd3c3a87ca36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a240e84d-954a-4b12-b894-141bcbfccdab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05d5f34e-b9b8-4e29-81e9-f4fa22ed618b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RPN(nn.Module):\n",
    "    def __init__(self, in_channels=512, mid_channels=512, n_anchor=9):\n",
    "        super(RPN, self).__init__()\n",
    "        self.mid_channels = mid_channels\n",
    "        self.in_channels = in_channels # depends on the output feature map. in vgg 16 it is equal to 512\n",
    "        self.n_anchor = n_anchor # Number of anchors at each location\n",
    "        self.conv1 = nn.Conv2d(self.in_channels, self.mid_channels, 3, 1, 1)\n",
    "        self.reg_layer = nn.Conv2d(mid_channels, n_anchor *4, 1, 1, 0) \n",
    "        self.cls_layer = nn.Conv2d(mid_channels, n_anchor *2, 1, 1, 0)\n",
    "        \n",
    "        self.conv1.weight.data.normal_(0, 0.01) # conv sliding layer\n",
    "        self.conv1.bias.data.zero_()\n",
    "        self.reg_layer.weight.data.normal_(0, 0.01) # Regression layer\n",
    "        self.reg_layer.bias.data.zero_()\n",
    "        self.cls_layer.weight.data.normal_(0, 0.01) # classification layer\n",
    "        self.cls_layer.bias.data.zero_()\n",
    "    \n",
    "    def forward(self, k):\n",
    "        bat_num = k.shape[0]\n",
    "        x = self.conv1(k)\n",
    "        pred_anchor_locs = self.reg_layer(x)\n",
    "        pred_cls_scores = self.cls_layer(x)\n",
    "        \n",
    "        pred_anchor_locs = pred_anchor_locs.permute(0, 2, 3, 1).contiguous().view(bat_num, -1, 4)\n",
    "        pred_cls_scores = pred_cls_scores.permute(0, 2, 3, 1).contiguous()\n",
    "        objectness_score = pred_cls_scores.view(bat_num, 50, 50, 9, 2)[:, :, :, :, 1].contiguous().view(bat_num, -1)\n",
    "        pred_cls_scores  = pred_cls_scores.view(bat_num, -1, 2)\n",
    "        \n",
    "        return pred_anchor_locs, pred_cls_scores, objectness_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70883f90-49fb-45bd-a848-7bb15590653c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
