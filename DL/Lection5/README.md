# Лекция №5


```python
from torch import nn
```

## Задачи DL кроме классификации(модификации)

- классификация+локализация(определить тип и показать расположение)
- семантическая сегментация(определение принадлежности пикселя тому или иному объекту)

### Локализация + классификация

- кроме возвращаемой метки класса слои выделяют расположение объекта
- детектирующие слои работают с выходами сверток до этапа классификации

### Метрика качества сегментации(детекторов)

- __IoU__: $IoU = \frac{|A\cap B|}{|A\cup B|} \ge \alpha$
- __MeanAvragePrecision__:
  - берем один класс;
  - смотрим, как много объектов этого класса выбраны алгоритмом правильно;
  - считаем prec, recall для каждой строки(объекта данных);
  - по результатам строим кривую, затем считаем площадь под графиком;
  - получаем качества для всех классов из датасета, усредняем и получаем MAP.

### Детектирование объектов

- __Selective search__:
  1. разбиваем изображение на сегменты(например, кластеризируем);
  2. каждый сегмент выделяем в рамку(получаем много рамок);
  3. сегменты объединяем в большие сегменты;
  4. повторяем п.2-3;
  5. получаем разбиение.

#### R-CNN

__Изначальная вариация__:
- Исходная картинка пропускается через SelectiveSearch;
- Убрали рамку, растянули изображение до нужных размеров($224\times224$);
- Классифицировали объект+SVM(+regression для коррекции SelectiveSearch);

__Дообучение на конкретном датасете__:
- Рассматривается объект на изображении;
- Если IoU>30%, то объект - машина(например), иначе - фон;

__BBox regression__:
- Известны параметры рамки(левый верхний угол и высота+ширина);
- Ошибка - как сильно ошиблись в выборе середины рамки, ширины и высоты;

__Недостатки__:
- Долгое обучение;
- CNN не для всего изображения, а для сегментов(много запусков).

__NMS__:
- Выбирается класс, берутся степени уверенности bbox'ов;
- Из них выбираются наибольшие уверенности:
  - Если пересечение большое, то новую рамку(меньше уверенность) зануляем(игнорируем);
  - Если пересечение незначительное, то оставляем рамку.

###

#### SPP-net: Spatial Pyramid Pooling

- Проблема классического детектирования - выбираем фиксированные размеры рамки, переводим в нужное разрешение, передаем в слои;
- __Можно__ поменять последовательность: передать изображение в слои и затем применить __pyramid pooling__.

__SPP__:
- Применим разное количество делений изображения(исходное, 4 части, 16 частей, например), затем собрать полученные данные и передать в сеть. В результате __работает быстрее__.


```python
import math

# Теперь есть adaptive pooling, 
# показывая желаемую размерность на выходе.
def spatial_pyramid_pool(self,previous_conv, num_sample, previous_conv_size, out_pool_size):
    '''
    previous_conv: a tensor vector of previous convolution layer
    num_sample: an int number of image in the batch
    previous_conv_size: an int vector [height, width] of the matrix features size of previous convolution layer
    out_pool_size: a int vector of expected output size of max pooling layer
    returns: a tensor vector with shape [1 x n] is the concentration of multi-level pooling
    '''
    for i in range(len(out_pool_size)):
        h_wid = int(math.ceil(previous_conv_size[0] / out_pool_size[i]))
        w_wid = int(math.ceil(previous_conv_size[1] / out_pool_size[i]))
        h_pad = (h_wid*out_pool_size[i] - previous_conv_size[0] + 1)/2
        w_pad = (w_wid*out_pool_size[i] - previous_conv_size[1] + 1)/2
        maxpool = nn.MaxPool2d((h_wid, w_wid), stride=(h_wid, w_wid), padding=(h_pad, w_pad))
        x = maxpool(previous_conv)
        if(i == 0):
            spp = x.view(num_sample,-1)
        else:
            spp = torch.cat((spp, x.view(num_sample,-1)), 1)
    return spp
```

#### Fast R-CNN

- R-CNN + SPP;
- Ошибка - сумма ошибок классификации и детекции;
- Пропукалось изображение через CNN, выделялась рамка, затем к ней применялись RoIPooling, передаем в FC.

#### Faster R-CNN

- Убрали SelectiveSearch => RPN;

__RPN__:
- Выберем anchors для детекции, полагая, что именно они и только они могут быть детектированы;
- После CNN:
  - Для каждой ячейки в карте отклика нарисуем k anchors:
    - Квадрат, прямоугольник, повернутый прямоугольник, и т.д.;
    - Для каждой точки строим это множество возможных bbox'ов;
  - Для каждого региона сеть определяет: есть объект или нет + какая рамка у объекта.
- __Пример__: определить кота и дерево на снимке:
  1.  Разбиваем RoIPooling'ом изображение на 2 на 3 таблицу;
  2.  Каждой ячейке приписываем весь набор anchor'ов;
  3.  Нашли bbox, которые сильно пересекаются с изначальной разметкой => RPN говорит, что объект здесь + bbox в ходе регрессии может расширяться

- __RPN__ не заостряет внимание на объекте и старается просто искать рамку, а __после классификации рамка уже уточняется специально для каждого класса__.


```python
class RPN(nn.Module):
    def __init__(self, in_channels=512, mid_channels=512, n_anchor=9):
        super(RPN, self).__init__()
        self.mid_channels = mid_channels
        self.in_channels = in_channels # depends on the output feature map. in vgg 16 it is equal to 512
        self.n_anchor = n_anchor # Number of anchors at each location
        self.conv1 = nn.Conv2d(self.in_channels, self.mid_channels, 3, 1, 1)
        self.reg_layer = nn.Conv2d(mid_channels, n_anchor *4, 1, 1, 0) 
        self.cls_layer = nn.Conv2d(mid_channels, n_anchor *2, 1, 1, 0)
        
        self.conv1.weight.data.normal_(0, 0.01) # conv sliding layer
        self.conv1.bias.data.zero_()
        self.reg_layer.weight.data.normal_(0, 0.01) # Regression layer
        self.reg_layer.bias.data.zero_()
        self.cls_layer.weight.data.normal_(0, 0.01) # classification layer
        self.cls_layer.bias.data.zero_()
    
    def forward(self, k):
        bat_num = k.shape[0]
        x = self.conv1(k)
        pred_anchor_locs = self.reg_layer(x)
        pred_cls_scores = self.cls_layer(x)
        
        pred_anchor_locs = pred_anchor_locs.permute(0, 2, 3, 1).contiguous().view(bat_num, -1, 4)
        pred_cls_scores = pred_cls_scores.permute(0, 2, 3, 1).contiguous()
        objectness_score = pred_cls_scores.view(bat_num, 50, 50, 9, 2)[:, :, :, :, 1].contiguous().view(bat_num, -1)
        pred_cls_scores  = pred_cls_scores.view(bat_num, -1, 2)
        
        return pred_anchor_locs, pred_cls_scores, objectness_score
```

#### YouOnlyLookOnce

- Одна CNN;
- За один прямой проход находит объекты и их классы;

1. Берем изображение, делим на одинаковые объекты($7\times7$);
2. Предсказываем регионы, размеры bbox'а и класс;
3. В результате работы получаем тензор $7\times7\times30$:
   - первые 5 значений для 1-го класса;
   - вторые 5 для второго;
   - далее вероятность классов в данной ячейке с меньшими уверенностями.
4. Получаем принадлежность рамки тому или иному объекту(классу);
5. Детектируем те ячейки, центры которых попали в данную рамку;

__Недостатки__:
- В каждой ячейке центры максимум 2-х классов.

__+__:
- Быстрее работает.

#### YOLO v2

- SkipConnections;
- Убрали FC слои;
- Anchor'ы определяли с помощью k-means.

#### YOLO v3

- Детекция на разных масштаба(не только $7\times7$);
- Каскадная схема(чем глубже слой, тем сильнее дробление изображения);

#### SSD

> С чего YOLO v3 взяла идею

- Детекция сеток разного масштаба;
- Рассматривались по-прежнему фиксированные наборы anchor-box'ов;
- Находим, кто изображен и как меняется bbox;
- Для каждой ячейки(масштаба карты) определен набор bbox'ов;
- Помогает детектировать объекты разных размеров;
- В итоговом тензоре больше размерность(последняя), т.к. показывает уверенность bbox'ов разных размеров;

#### FPN

- Иерархический подход к детекции(разные объекты разных размеров);
- __Вариации__:
  - Каждый слой(изменить изображение, передать сети) дает свое предсказание;
  - Предсказание берется только с последнего слоя(причем через сеть проходит только исходное изображение);
  - SSD-вариант(иерархия с исходным изображением(т.е. не меняются размеры изображения), причем предсказание с каждого слоя);
  - SSD-вариант, но с обратным уточнением предсказаний, причем с каждого слоя берется предсказание;
  - Как и в предыдущем варианте, только предсказание с последнего слоя(наибольший размер карты) - U-Net.

### Новые подходы

#### Anchor-Free[без якорей]

- Пусть на изображении несколько объектов;
- Рамку определяют два угла(левый верхний и нижний правый);
- __Пусть__ каждый пиксель определяет, является ли он углом - тогда не нужны якоря(CornerNet; _CenterNet_ - центры рамки);
    - Изменился Pooling: 
      - Не все объекты в левом верхнем углу содержат точки объекта, поэтому заполняем сокращенную карту признаков максимумами в строке и в столбце;
    - __CornerNet__:
      - Предсказываются углы рамки и вероятность быть углом;
      - Embedding - кодируем(и характерные им углы) объекты для разделимости;
      - Offsets - смещение рамки для большей точности;
      - Больше одной точки считаются углом(окрестность).

#### FCOS

- Качество лучше большинства алгоритмов детекции.

- Пусть не углы определяются, а принадлежность пикселя объекту:
  - __Недостаток__: если несколько объектов, то одна точка может быть в двух объектах:
    - Если такая ситуация случилась, предпочтение минимальному региону.
- В каждом пикселе предсказываем расстояние до границ объекта и метка класса и вероятность, что объект в центре.

__Архитектура__:
- Как в _двунаправленной пирамиде_ + детекция(отдельно в head) - классификация, центральность, bbox.


