# Лекция №9

# Трансформеры

## Последовательности

 - __было__: последовательность через склееные RNN переводим в эти же последовательсности(seq2seq);
 - __Проблемы__:
   - никакого _внимания_;

- Идея: QueryKeyValue;

### SelfAttention

- __Attemntion__ - есть две матрицы:
  - ключи($d\times s$, $d$ - число примеров)
  - значения
  - есть некоторый запрос: $d$-мерный вектор
  - _Если схожесть - __скалярное произведение__, то_:
    - считается softmax - чем больше схожесть ключа с запросов, тем больше вклад слагаемого в сумму, так определяем предсказане;
- модификация _self_:
  - Запросы теперь матрица, а не вектор.
    - домножаем для получения значения не на вектор, а на всю матрицу запросов.

> head(несколько голов) через поиск близости столбцов(объектов)

- scale - домножение на нормировочную $\alpha$.
- в декодировщике multi-head-cross-attention позволяет использовать кодировщик в декодировщике;

- для чего multi-head?
  - каждый head смотрит на свои признаки(как свертки);
- для генерации звуков использовали $\le$ head'ов, т.к. эмперически качество ухудшается(теоретически обоснование лучшего расширения числа голов _на пальцах_)

### Positional Embedding

- помогает хранить пространственную(глобальную) информацию об объекте;

### LayerNorm

- прокидывание связей - прибавляем к результату исходный элемент;
- нормирование всего в слое(используя разные входы для разных модулей)

